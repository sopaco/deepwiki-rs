# System Boundary Interface Documentation

This document describes the system's external invocation interfaces, including CLI commands, API endpoints, configuration parameters, and other boundary mechanisms.

## Command Line Interface (CLI)

### litho

**Description**: Main entry point for the documentation generation tool. Analyzes source code and generates C4 architecture documentation using AI agents.

**Source File**: `src/main.rs`

**Arguments**:

- `config` (PathBuf): optional - Path to the configuration TOML file

**Usage Examples**:

```bash
litho
```

```bash
litho --config ./litho.toml
```

```bash
litho --config /path/to/custom-config.toml
```

### litho sync-knowledge

**Description**: Synchronizes external knowledge sources from local documentation files (Markdown, PDF, SQL, etc.) into the knowledge base for RAG-enhanced documentation generation. Checks cache state and performs chunked ingestion of documents.

**Source File**: `src/main.rs`

**Arguments**:

- `config` (PathBuf): optional - Path to the configuration TOML file (overrides top-level config if specified)
- `force` (bool): optional - Force synchronization even if the knowledge cache is up to date (default: `false`)

**Options**:

- `force`(flag): optional - Force sync bypassing cache validation (default: `false`)
- `config`(PathBuf): optional - Configuration file path

**Usage Examples**:

```bash
litho sync-knowledge
```

```bash
litho sync-knowledge --force
```

```bash
litho sync-knowledge --config ./litho.toml --force
```

## Integration Suggestions

### Configuration File Setup

Basic project integration using a configuration file. The tool expects a litho.toml file in the project root or accepts an explicit path via --config. Configuration includes LLM provider settings, project metadata, exclusion patterns, and knowledge base integration.

**Example Code**:

```
# litho.toml
project_name = "my-awesome-project"
project_path = "."
output_path = "./docs/architecture"
target_language = "en"

[llm]
provider = "openai"
api_key = "${LITHO_LLM_API_KEY}"  # From env var
api_base_url = "https://api.openai.com/v1"
model_efficient = "gpt-4o-mini"
model_powerful = "gpt-4o"
max_tokens = 4096
temperature = 0.1
retry_attempts = 3

[cache]
enabled = true
cache_dir = ".litho/cache"
expire_hours = 8760  # 1 year

[knowledge]
[knowledge.local_docs]
enabled = true
watch_for_changes = true

[[knowledge.local_docs.categories]]
name = "architecture"
description = "Architecture decision records and design docs"
paths = ["docs/adr/*.md", "docs/design/*.md"]
target_agents = ["architecture_researcher"]

[[knowledge.local_docs.categories]]
name = "database"
description = "Database schemas and migration scripts"
paths = ["db/**/*.sql", "migrations/**/*.sql"]
target_agents = ["database_analyzer"]
```

**Best Practices**:

- Store the litho.toml configuration file in version control to share settings across team members
- Use environment variables (LITHO_LLM_API_KEY) for sensitive API keys rather than hardcoding in config files
- Set up a .gitignore entry for the .litho/ directory to avoid committing cache files
- Configure specific file extensions and exclusion patterns to reduce analysis time on large projects
- Use the --force flag sparingly in CI/CD to avoid unnecessary API calls and processing time

### LLM Provider Configuration

Deepwiki-rs supports 8 LLM providers including OpenAI, Anthropic, Google Gemini, Moonshot, DeepSeek, Mistral, OpenRouter, and local Ollama instances. Configuration supports dual-model setup with automatic fallback from efficient to powerful models.

**Example Code**:

```
# For OpenAI
[llm]
provider = "openai"
api_key = "sk-..."
model_efficient = "gpt-4o-mini"
model_powerful = "gpt-4o"

# For Ollama (local inference)
[llm]
provider = "ollama"
api_base_url = "http://localhost:11434"
model_efficient = "llama3.1:8b"
model_powerful = "llama3.1:70b"

# For DeepSeek
[llm]
provider = "deepseek"
api_key = "${DEEPSEEK_API_KEY}"
api_base_url = "https://api.deepseek.com/v1"
model_efficient = "deepseek-chat"
model_powerful = "deepseek-reasoner"
```

**Best Practices**:

- Use ModelScope or Ollama for air-gapped environments requiring no external internet access
- Configure dual-model strategy: efficient model for routine analysis, powerful model for complex architectural inference
- Set appropriate retry_attempts and retry_delay_ms for rate-limited APIs (5 attempts with 5s delays recommended)
- Disable preset tools (disable_preset_tools = true) if you want full manual control over agent capabilities
- Use Ollama for local development to avoid API costs during testing

### Knowledge Base Integration

Integration of external knowledge sources enables RAG (Retrieval-Augmented Generation) for context-aware documentation. Supports Markdown, SQL, and PDF files categorized by domain (architecture, database, API) with semantic chunking for large documents.

**Example Code**:

```
# In CI/CD pipeline (GitHub Actions example)
- name: Cache Litho Knowledge Base
  uses: actions/cache@v3
  with:
    path: .litho/cache
    key: litho-${{ hashFiles('docs/**/*.md', 'db/**/*.sql') }}

- name: Sync External Knowledge
  run: litho sync-knowledge --force
  env:
    LITHO_LLM_API_KEY: ${{ secrets.LLM_API_KEY }}

- name: Generate Documentation
  run: litho --config litho.ci.toml

# Category configuration for domain-specific knowledge
[[knowledge.local_docs.categories]]
name = "api"
description = "External API specifications"
paths = ["api-specs/*.yaml", "openapi/*.json"]
target_agents = ["boundary_analyzer"]

[knowledge.local_docs.categories.chunking]
enabled = true
strategy = "semantic"
max_chunk_size = 8000
chunk_overlap = 200
```

**Best Practices**:

- Run litho sync-knowledge in CI before documentation generation to ensure external docs are indexed
- Cache the .litho/ directory between CI runs to speed up subsequent executions
- Use --force only when documentation sources change to avoid unnecessary processing
- Organize knowledge base documents by category with specific target_agents for optimal context injection
- Enable watch_for_changes only in development environments, not in CI

### CI/CD Pipeline Integration

Deepwiki-rs is designed for CI/CD integration to automatically maintain up-to-date architecture documentation. The tool exits with appropriate error codes for pipeline failure detection and supports non-interactive execution.

**Example Code**:

```
# GitLab CI example
generate-docs:
  stage: documentation
  image: rust:latest
  before_script:
    - cargo install deepwiki-rs
  script:
    # Sync knowledge if external docs changed
    - litho sync-knowledge
    # Generate documentation
    - litho --config litho.toml
  artifacts:
    paths:
      - litho.docs/
    expire_in: 1 month
  only:
    - main
    - tags

# Advanced configuration for CI environments
# litho.toml
include_tests = false
include_hidden = false
max_file_size = 65536  # 64KB limit for CI speed

[llm]
max_parallels = 5  # Increase parallelism in CI environment
timeout_seconds = 120  # Shorter timeout for CI

# Exclude CI-generated files
excluded_files = ["*.lock", "*.log", ".env.ci", "*.tmp"]
```

**Best Practices**:

- Add documentation generation as a scheduled CI job (weekly) rather than per-commit to manage costs
- Commit generated docs to a separate branch or documentation repository
- Use include_tests = false and exclude build artifacts to reduce token consumption
- Set max_file_size limit to prevent processing large generated files
- Generate docs for release tags only to maintain stable documentation versions
- Validate generated docs with markdown linters in the CI pipeline


---

**Analysis Confidence**: 8.0/10
